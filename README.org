#+TITLE: Kafka disaster recovery with MirrorMaker 2
#+OPTIONS: toc:nil

* Kafka disaster recovery with MirrorMaker 2

#+TOC: headlines

* Introduction

This repository includes example file for creating a disaster safe Kafka cluster.
The example configuration is

- a main-kafka cluster deployed with strimzi (https://strimzi.io)
- a dr-kafka cluster deployed with strimzi (https://strimzi.io)
- a MirrorMaker2 configuration to sync topics from main-kafka to dr-kafka

* Getting started

For installating the strimzi operator follow the [[https://strimzi.io/docs/operators/master/quickstart.html#proc-install-product-str][quick-start
installation]] documention.  We used the strimzi operator bundled with
Red Hat's AMQ streams. But the examples below should also work with
the upstream operator. AMQ Streams 1.5 comes with Kafka 2.5 and
Strimzi 0.18.0.

* Examples

** Single instance Kafka

In the [[file:examples/single-kafka][examples/single-kafka]] folder is a very simple single kafka
configuration for getting started with the strimzi operator. It creates the following objects

- A main-kafka [[file:examples/single-kafka/10-main-kafka-namespace.yml][namespace]] for running the main kafka instance
- A strimzi kafka [[file:examples/single-kafka/20-main-kafka.yml][resource]] for running Kafka
- A strimzi topic [[file:examples/single-kafka/30-topic.yml][resource]] for creating a test topic
- A test [[file:examples/single-kafka/40-test-producer.yml][producer]] that uses the main-kafka Kafka instance
- A test [[file:examples/single-kafka/50-test-consumer.yml][consumer]] that uses the main-kafka Kafka instance

** DR Kafka configuration

For testing the desaster recovery safe Kafka configuration we create a
second namespace dr-kafka and mirror all topics from main-kafka with
MirrorMaker 2 to this instance.

We are using the same resouces as in [[Single instance Kafka]]. Additionally we create

* Route sharding

We would like to expose routes created by strimzi to a separate ingress controller (aka router).
The are a little bit different between OpenShift 4 and 3.11.

** OpenShift 4.n

Create an ingress controller with the following options:

#+begin_src yaml
    routeSelector:
      matchLabels:
        strimzi.io/kind: Kafka
#+end_src

A example configuration is [[file:ingress/kafka-ingress.yml][here]]

This ingress controller will select routes with a label
~strimzi.io/kind: Kafka~ . If you want to publish routes only from a
specific Kafka cluster on this ingress controller you could also use
the label ~strimzi.io/cluster: main-kafka~. But remember that you have
to change the route selector for the default ingress controller as
well (see below).

We also do not want to publish kafka routes on the default ingress controller so we change the default configuration
with

#+begin_src sh
oc edit ingresscontrollers.operator.openshift.io default  -n openshift-ingress-operator
#+end_src

and add the following stanza

#+begin_src yaml
spec:
  routeSelector:
    matchExpressions:
    - key: strimzi.io/kind
      operator: NotIn
      values:
      - Kafka
#+end_src

so the default ingress controller will _not_ pick up routes with the label ~strimzi.io/kind: Kafka~.

** OpenShift 3.11 (untested)

*WARNING*: This is untested because no 3.11 cluster was available.

According to the route sharding docs at
[[https://docs.openshift.com/container-platform/3.11/install_config/router/default_haproxy_router.html#using-router-shards]]
you have to use environment variables to modify the router
configuration.

One problem is how to exclude routes created for Kafka from the
default router. A possible solution is to expose kafka in a separate
subdomain and use the environment variable ~ROUTER_DENIED_DOMAINS~ in
the default router so it does *not* pick up routes for kafka.

We would propose the following steps:

1. Create a new router template with ~oc adm router --dry-run -o yaml --service-account=router > kafka-router.yml~
2. Modify the generated yaml file
   - change the router name
   - add the following environment variable ~ROUTE_LABELS='strimzi.io/kind=Kafka'~ OR
   - use ~ROUTER_ALLOWED_DOMAINS~, so that the kafka router only picks up routes for a certain domain
     e.g. ~oc set env dc/router ROUTER_ALLOWED_DOMAINS=kafka.ocp3.local~, if more than one domain is used they should be separated by a comma.
3. Create the router with ~oc create -f kafka-router.yml and test if it picks up the kafka routes
4. Modify the default router so it does not expose routes for the Kafka domain ~oc set env dc/router ROUTER_DENIED_DOMAINS=kafka.ocp3.local~
